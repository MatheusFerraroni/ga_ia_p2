{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h8RU2F2yxsoU"
   },
   "source": [
    "<img src=\"images for report/unicamp.png\" width=\"150\" height=\"150\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uDgcJ3xfSwxh"
   },
   "source": [
    "# Introduction to Artificial Intelligence - MO416A\n",
    "\n",
    "\n",
    "This work was completed by the following members:\n",
    "\n",
    "\n",
    "\n",
    "*   Aissa Hadj - 265189\n",
    "*   Lucas Zanco Ladeira - 188951\n",
    "*   Matheus Ferraroni - 212142\n",
    "*   Maria Vitória Rodrigues Oliveira - 262884\n",
    "*   Oscar Ciceri - 164786\n",
    "\n",
    "The original code of the project is located on a [repository inside Github](https://github.com/lucaslzl/ga_ia_p2) and the video showing the search strategies working is on [youtube](https://youtube.com). \n",
    "\n",
    "\n",
    "\n",
    "# Genetic Algorithm for Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vsyf19A0q2sQ"
   },
   "source": [
    "# I - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H8PUV6MgqRvZ"
   },
   "source": [
    "The problem that will be tackled in this project is Feature Selection. The goal is to obtain the subset of features from a dataset that gives the best model performance in terms of accuracy and error rate. Also, the presence of irrelevant features, in the dataset, requires more processing and memory powers than necessary, thus wasting computing resources. To better understand the possible impacts of feature selection, we could cite the following pros: \n",
    "\n",
    "- reducing overfitting\n",
    "- improving the model accuracy\n",
    "- reducing training time\n",
    "\n",
    "Feature Selection may be done manually or by the computer. Some manual techniques include univariate selection, feature importance, and the correlation matrix. The objective of the univariate selection method is to statistically describe the relationship between each feature and the target. Also, feature importance generates a score for each feature to rank it. For instance, Decision Tree algorithms may rank features according to Gini impurity tests. Finally, the correlation matrix shows the correlation between pairs of features so that the features with a high correlation could be removed. The literature presents the usage of optimization techniques to automatically find the best (or a quite good) subset of features. Some of the methods include:\n",
    "\n",
    "- <b>Exhaustive search</b>\n",
    "- <b>Simulated Annealing</b>\n",
    "- <b>Transformation Graph</b>\n",
    "- <b>Genetic Algorithms</b>\n",
    "\n",
    "<b>Exhaustive Search</b> is not an optimization technique, but it is worth to be mentioned as its computational complexity is $O(2^n)$. This technique tries every possible subset of features to find the best one. Due to its computational complexity, this technique is not practical in most cases. <b>Simulated Annealing</b> is a metaheuristic for complex nonlinear optimization problems and is analogous to the simulation of the annealing of solids. The analogy pairs are as follows: feasible solution (state), cost (energy), optimal solution (ground state), local search (rapid quenching), simulated annealing (careful annealing). On the other hand, <b>Transformation Graph</b> is a strategy that utilizes a tree-like structure to generate possible solutions. First, $n$ solutions are generated by removing at each one distinct feature. Second, all the solutions are evaluated. Third, the best solution is chosen and $n-1$ is generated by removing each feature yet not removed. That strategy goes on considering a budget. The issue of this strategy is that it requires a substantial amount of memory. Finally, <b>Genetic Algorithm</b> is inspired by genetics (DNA) to search through solutions. Its process can be described with the following steps: (1) it generates a population considering variations of the DNA, (2) then ranks the population according to some score, it applies some forms of mutations and other transformations to the DNA at each generation, and finally, (3) it iterates through the previous steps until it reaches a stopping rule. The stopping rule may be the number of generations produced or a target solution was reached. \n",
    "\n",
    "In this project, we decided to only tackle the Genetic Algorithm strategy as it comprehends a big ground to cover already. That said, this report is structured as follows: in Section 2, we describe the implementation of the main parts of the genetic algorithm; in Section 3, we discuss the methodology we followed to undertake this project; in Section 4, we do a detailed analysis of the results; and finally, in Section 5 we present the conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sW3ewDHPqRvc"
   },
   "source": [
    "# II - Genetic Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZPkoG23yqRvd"
   },
   "source": [
    "We developed 2 classes to control the operation of the genetic algorithm. The classes are 'element' and 'GeneticAlgorithm' and can be seen in https://github.com/lucaslzl/ga_ia_p2/blob/master/GA.py. In this notebook, the class 'element' will be explained, as well as the main methods of the class 'GeneticAlgorithm'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YEgj38W2qRvd"
   },
   "source": [
    "Below, the entire class 'element' is presented. It is possible to see that this class is only responsible for managing the id, the generation, the genome, and the score of each element of the population for every generation. Saving these attributes in the same place can be useful for different approaches during the implementation and use of the methods on the genetic algorithm. It is also possible to save the parents of each element and traceback how each element was formed during the evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JYT_fL3wqRve"
   },
   "outputs": [],
   "source": [
    "class element:\n",
    "\n",
    "    def __init__(self, idd, geracao, genome):\n",
    "        self.idd = idd\n",
    "        self.geracao = geracao\n",
    "        self.genome = genome\n",
    "        self.score = None\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"(id=\"+str(self.idd)+\",geracao=\"+str(self.geracao)+\",score=\"+str(self.score)+\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AwQyOY1wqRvk"
   },
   "source": [
    "The genetic algorithm implemented is very generic, this means that it can be applied easily to different problems. Using this kind of generic implementation makes it practical to override the functions responsible to generate a random genome, to mutate, and perform the fitness calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ncTpcU_SqRvl"
   },
   "source": [
    "The method 'create_initial_population' is called once the genetic algorithm is launched. This method is responsible for creating the individuals of the initial population and add them to the pool until the required population size is reached. To create the genome, the function 'random_genome', that was overwritten before, is called for each element.\n",
    "\n",
    "In our problem, we define the genome as an array of 0's and 1's as elements with a length equal to the number of features in the dataset. This kind of approach used in the genome allows us to decode the genome as:\n",
    "\n",
    "- Every bit of the genome corresponds to a feature in the dataset\n",
    "- If the bit is 1: the feature corresponding to that bit is used\n",
    "- If the bit is 0: the feature corresponding to that bit isn't used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fx3_fIfgqRvl"
   },
   "outputs": [],
   "source": [
    "def create_initial_population(self):\n",
    "    for _ in range(self.population_size):\n",
    "        self.population.append(element(self.elements_created, 0, self.random_genome()))\n",
    "        self.elements_created += 1\n",
    "\n",
    "\n",
    "def random_genome():\n",
    "    return np.random.randint(low=0,high=2,size=len(df.columns),dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yE4dvXf_dEqo"
   },
   "source": [
    "The method 'run' is where the main loop of the genetic algorithm is executed. The steps are:\n",
    "\n",
    "1. Check the stop criteria\n",
    "2. Calculate the score for the current population\n",
    "3. Sort the population according to the fitness value\n",
    "4. Update the solution if a better result was found\n",
    "5. Save the log\n",
    "6. If set, part of the worst part the of the population can be discarded (This is not being used in the solutions found for this work)\n",
    "7. Create a new population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_mIDkEOhqRvp"
   },
   "outputs": [],
   "source": [
    "def run(self):\n",
    "\n",
    "    while self.check_stop():\n",
    "        self.calculate_score() \n",
    "        self.population.sort(key=lambda x: x.score, reverse=True) \n",
    "\n",
    "        if self.best_element_total==None or self.population[0].score > self.best_element_total.score: \n",
    "            self.best_element_total = self.population[0]\n",
    "\n",
    "        self.do_log()\n",
    "\n",
    "        if self.cut_half_population: \n",
    "            self.population = self.population[0:len(self.population)//2] \n",
    "\n",
    "        self.new_population()\n",
    "\n",
    "        self.iteration_counter +=1\n",
    "\n",
    "    return self.best_element_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GhQ2dBOcdEqt"
   },
   "source": [
    "The creation of a new population is implemented to be the same independently of how the selection of the parents is made. To achieve this, we create an array of probability that implements the rules for this selection based on the equal chance of each element or by roulette, where elements with higher fitness have higher chances to be selected.\n",
    "\n",
    "The crossover method receives the parent's genome and returns a new genome-based on its genome.\n",
    "This new genome is inserted into the new element and this new element is added to the new pool.\n",
    "\n",
    "If we are recreating the entire population, this process repeats until the new population has the size of the population size limit. If we are replicating a percentage of the best elements, the amount of best elements being replicated is reduced in this process and they are replicated after this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C0KNu6B_qRvs"
   },
   "outputs": [],
   "source": [
    "def new_population(self):\n",
    "\n",
    "    probs = self.get_probs()\n",
    "    newPop = []\n",
    "    best_replicator = int(self.population_size*self.replicate_best)\n",
    "\n",
    "    while len(newPop)<self.population_size-best_replicator:\n",
    "        parents = np.random.choice(self.population,size=2,p=probs) \n",
    "\n",
    "        if parents[0].score<parents[1].score: \n",
    "            parents = parents[::-1] \n",
    "\n",
    "        new_element = element(self.elements_created, self.iteration_counter, self.crossover(parents[0].genome, parents[1].genome))\n",
    "\n",
    "        new_element.genome = self.active_mutate(new_element.genome)\n",
    "        newPop.append(new_element)\n",
    "        self.elements_created += 1\n",
    "\n",
    "    for i in range(best_replicator):\n",
    "        newPop.append(self.population[i])\n",
    "\n",
    "    self.population = newPop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JyMBfzT1dEq1"
   },
   "source": [
    "In order to define the probability of being selected as a parent, we implement 3 methods: \"get_probs\", \"probs_equal\" and \"probs_roulette\".\n",
    "\n",
    "The \"get_probs\" just checks what kind of probability function must be used and calls the right one.\n",
    "The \"probs_equal\" returns an array of probability where every element has the same chance of being selected.\n",
    "The \"probs_roullete\" adds the fitness of each element to the array, sums its total, and divides the array by this sum. The result is an array where the best solutions have higher chances of being selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xrkGFw9GqRvw"
   },
   "outputs": [],
   "source": [
    "def get_probs(self):\n",
    "    if self.probs_type == 0:\n",
    "        return self.probs_roulette()\n",
    "    elif self.probs_type == 1:\n",
    "        return self.probs_equal()\n",
    "\n",
    "\n",
    "def probs_equal(self):\n",
    "    return [1/len(self.population)]*len(self.population)\n",
    "\n",
    "\n",
    "def probs_roulette(self):\n",
    "    probs = [0]*len(self.population) \n",
    "    for i in range(len(probs)):\n",
    "        probs[i] = self.population[i].score\n",
    "    div = sum(probs)\n",
    "\n",
    "    if div!=0:\n",
    "        for i in range(len(probs)):\n",
    "            probs[i] /= div\n",
    "    else: \n",
    "        probs = self.probs_equal()\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kEPu0GC7dEq6"
   },
   "source": [
    "The method \"check_stop\" is responsible for checking which stops criteria must be used. The stop criteria can be used in 3 different ways.\n",
    "\n",
    "The \"stop_criteria_iteration\" just returns \"True\" if a minimum amount of iterations has been attained.\n",
    "The \"stop_criteria_score\" just returns \"True\" if any solution for a given generation achieved a minimum score.\n",
    "The \"stop_criteria_double\" is a mix of the previous 2 methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I3zwbZjtqRvz"
   },
   "outputs": [],
   "source": [
    "def check_stop(self):\n",
    "    if self.stop_criteria_type==0:\n",
    "        return self.stop_criteria_double()\n",
    "    elif self.stop_criteria_type==1:\n",
    "        return self.stop_criteria_iteration()\n",
    "    elif self.stop_criteria_type==2:\n",
    "        return self.stop_criteria_score()\n",
    "\n",
    "def stop_criteria_double(self):\n",
    "    s = self.population[0].score\n",
    "    if s==None:\n",
    "        s = 0\n",
    "    return self.iteration_counter<self.iteration_limit or s>=self.max_possible_score\n",
    "\n",
    "def stop_criteria_iteration(self):\n",
    "    return self.iteration_counter<self.iteration_limit\n",
    "\n",
    "def stop_criteria_score(self):\n",
    "    s = self.population[0].score\n",
    "    if s==None:\n",
    "        s = 0\n",
    "    return s>=self.max_possible_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p4V5JOLddEq-"
   },
   "source": [
    "We implemented 4 different crossovers and 1 function to decide which one must be used. The function \"crossover\" just receives the genome of 2 parents and calls the right genome function.\n",
    "\n",
    "The \"crossover_rate_selection\" iterates the genome of both parents and selects from which one the bit must be selected. To define which one to select, this function checks a percentage that was defined previously. This means that the resulting genome can be 80% from one parent and 20% from the other, where the 80% comes from a parent with the highest score.\n",
    "\n",
    "The \"crossover_uniform\" just selects the bits from the parent with a chance close to 50%/50% from the selection from each parent.\n",
    "\n",
    "The \"crossover_single_point\" defines a random point in the middle of the genome and picks the first part from parentA and the second part from the parentB.\n",
    "\n",
    "The \"crossover_two_point\" defines 2 random points in the middle of the genome and concatenates a part of the genomeA, a part from genomeB and a part from the genomeA using the points defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0eAPc2T5qRv3"
   },
   "outputs": [],
   "source": [
    "def crossover(self, genA, genB):\n",
    "    if self.crossover_type==0:\n",
    "        return self.crossover_uniform(genA, genB)\n",
    "    elif self.crossover_type==1:\n",
    "        return self.crossover_single_point(genA, genB)\n",
    "    elif self.crossover_type==2:\n",
    "        return self.crossover_two_point(genA, genB)\n",
    "    elif self.crossover_type==3:\n",
    "        return self.crossover_rate_selection(genA, genB)\n",
    "\n",
    "def crossover_rate_selection(self, genA, genB):\n",
    "    new = np.array([],dtype=int)\n",
    "    for i in range(len(genA)):\n",
    "        if np.random.random()<self.crossover_rate:\n",
    "            new = np.append(new, genA[i])\n",
    "        else:\n",
    "            new = np.append(new, genB[i])\n",
    "    return new\n",
    "\n",
    "\n",
    "def crossover_uniform(self, genA, genB):\n",
    "    new = np.array([],dtype=int)\n",
    "    for i in range(len(genA)):\n",
    "        if np.random.random()<0.5:\n",
    "            new = np.append(new, genA[i])\n",
    "        else:\n",
    "            new = np.append(new, genB[i])\n",
    "    return new\n",
    "\n",
    "\n",
    "def crossover_single_point(self, genA, genB):\n",
    "    p = np.random.randint(low=1,high=len(genA)-1) \n",
    "    return np.append(genA[0:p],genB[p:])\n",
    "\n",
    "\n",
    "def crossover_two_point(self, genA, genB):\n",
    "    c1 = c2 = np.random.randint(low=0,high=len(genA)) \n",
    "    while c2==c1: \n",
    "        c2 = np.random.randint(low=0,high=len(genA))\n",
    "\n",
    "    if c1>c2: \n",
    "        c1, c2 = c2,c1\n",
    "\n",
    "    new = np.append(np.append(genA[0:c1],genB[c1:c2]),genA[c2:]) \n",
    "    return new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SaY4R1d4dErC"
   },
   "source": [
    "The method \"calculate_score\" was implemented to be used synchronously or asynchronously and the user can define how to execute it before the start of the main loop.\n",
    "\n",
    "This function just passes the genome of each element to a function that returns its fitness. In our case, we call the function \"evaluate\" that is responsible to decode the genome to use or not the features in the dataset and to check how good these features perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aZ6K_G3NqRv6"
   },
   "outputs": [],
   "source": [
    "def calculate_score(self):\n",
    "    if self.use_threads: \n",
    "\n",
    "        threads_running = []\n",
    "        for e in self.population:\n",
    "            x = threading.Thread(target=self.thread_evaluate, args=(e,))\n",
    "            x.start()\n",
    "            threads_running.append(x)\n",
    "\n",
    "        for i in range(len(threads_running)):\n",
    "            threads_running[i].join()\n",
    "\n",
    "    else: \n",
    "        for e in self.population:\n",
    "            e.score = self.evaluate(e.genome)\n",
    "\n",
    "def thread_evaluate(self, e):\n",
    "    e.score = self.evaluate(e.genome)\n",
    "    \n",
    "\n",
    "def evaluate(genome):\n",
    "    bool_genome = list(map(bool, genome))\n",
    "    return model.evaluate(df.loc[:, bool_genome].copy(), target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Sy4V2b8dErF"
   },
   "source": [
    "The mutation method is called \"active_mutate\" and receives a single genome. This method iterates through the entire genome and creates a random value if this value is smaller than the one set in the initialization, then a mutation is started on that index.\n",
    "\n",
    "We implemented 2 different mutations for this project:\n",
    "\n",
    "- mutate1: This method implements the generative mutation, which randomly changes a gene.  The genes have binary values; thus, the selected gene changes the allele value for his complement.\n",
    "\n",
    "\n",
    "- mutate2: This method implements the sequence swap combined with a generative mutation. First, a random position of the gene on a chromosome is selected. The genes located after this position are move to the beginning on the chromosome, and genes located before are move to the last. Moreover, the generative mutations technique is employed in the new chromosome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DXlHRcy5qRv-"
   },
   "outputs": [],
   "source": [
    "def active_mutate(self,gen):\n",
    "    if self.mutation_rate<=0: \n",
    "        return gen\n",
    "    for i in range(len(gen)): \n",
    "        if np.random.random()<self.mutation_rate: \n",
    "            gen = self.mutate1(i, gen) \n",
    "    return gen\n",
    "\n",
    "\n",
    "def mutate1(index, genome):\n",
    "    if genome[index]==0:\n",
    "        genome[index] = 1\n",
    "    else:\n",
    "        genome[index] = 0\n",
    "    return genome\n",
    "\n",
    "\n",
    "def mutate2(index, genome):\n",
    "    aux = []\n",
    "    for i in range(len(genome)):\n",
    "        if i <= index:\n",
    "            aux.append(genome[i])\n",
    "        else:\n",
    "            aux.insert(0, genome[i])\n",
    "    genome = aux\n",
    "    if genome[index]==0:\n",
    "        genome[index] = 1\n",
    "    else:\n",
    "        genome[index] = 0\n",
    "    return genome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o8OBSNg3qRwA"
   },
   "source": [
    "# III - Methodology\n",
    "\n",
    "As we already mentioned before, in this project, we apply the genetic algorithm (GA) to help us select a subset of features. The selection may help a data scientist in analyzing the data and improving the efficiency of machine learning models. To evaluate the GA strategy, a supervised learning model was chosen. The model contains a <i>target</i> that is the value from the dataset that must be predicted. Supervised learning models may be divided into two groups: \"Classification\", which is the prediction of a categorical feature (class); and \"Regression\", which consists of the prediction of a continuous numeric value. In this work, we tackle classification problems only. Also, we only selected tabular data since there are a huge amount of them freely available. Moreover, there are well known pre-processing techniques that are relatively easy to use. In one of the datasets, the task is classifying mushrooms according to their characteristics. Finally, we perform some variations on the GA parameters to evaluate its behaviour and compare the final subsets of features obtained from each configuration. \n",
    "\n",
    "\n",
    "Below, we give more details about the methodology we followed to undertake this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jv8RVuMf2xFL"
   },
   "source": [
    "## 1 - Tabular Data\n",
    "\n",
    "We use the term \"tabular data\" to designate datasets composed of features that could take numerical values (for instance the income amount of a client with a checking account in a bank institution) or categorical values (\"Yes\" or \"No\" values describing if a client has a membership in a mileage program from an airline company for example). We chose to use only tabular data for some practical reasons, as such:\n",
    "- There is a huge number of datasets available at [Kaggle](https://www.kaggle.com/), [OpenML](https://www.openml.org/), [UCI](https://archive.ics.uci.edu/ml/datasets.php).\n",
    "- They are relatively easy to pre-process and clean.\n",
    "- There are packages (i.e., Pandas) that are well suited to work with tabular data.\n",
    "- In the literature, they are the focus of the feature selection task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hho_YsmexspD"
   },
   "source": [
    "## 2 -  Datasets\n",
    "\n",
    "Bearing in mind that the datasets must be of a reasonable size (up to about 200MB) due to the limitation of computer power available at our disposal, the focus being on classification tasks, and the features being either numerical or categorical, a total of 9 different datasets were selected. The majority of them comes from the competition website Kaggle. The number of features from each dataset varies from 9 to over 30. A dataset containing more than 500 features was initially chosen but we ended up discarding it since it required a huge amount of time to simulate. That will help us evaluate the computing resources needed by the genetic algorithm, and its practicality in applying it to the real world. In particular, as we will see in more details in the results section, as we increase the number of features collected in a given dataset, the number of observations needed to build a relatively \"good\" machine learning model increases. This issue is termed as \"the curse of dimensionality\". We mention here the importance in varying the number of features since it impacts tremendously the amount of resources needed by the genetic algorithm in terms of time and computing power. The exact figures are shown in the table below.\n",
    "\n",
    "Dataset name | Number of features | All possible feature combinations\n",
    "--------- | --------- | --------- \n",
    "Glass | 9 | 512\n",
    "Cellphone | 20 | 1,048,576\n",
    "Mushrooms | 22 | 4,194,304\n",
    "Airline Customer Satisfaction | 22 | 4,194,304\n",
    "Kobe | 24 | 1,677,7216\n",
    "Flag | 29 | 536,870,912\n",
    "IBM | 34 | 17,179,869,184\n",
    "Band | 37 | 137,438,953,472\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D0cXeDiuy2xx"
   },
   "source": [
    "## 3 - Classification Model\n",
    "\n",
    "As the python language was used in this work, some libraries that already implemented machine learning models may be mentioned:\n",
    "- [Scikit-learn](https://scikit-learn.org/stable/).\n",
    "- [TensorFlow](https://www.tensorflow.org/).\n",
    "- [PyTorch](https://pytorch.org/).\n",
    "\n",
    "In these libraries, many classification models are available, as such: Neural Networks, Decision Trees, Support Vector Machines and more. The Genetic Algorithm has to train and test a model for each member of the population within each generation. That implies the necessity to choose a lightweight model according to the time at our disposal and the available computing power. With that said, Neural Networks, although they usually have a good result, demand a large computing power to train. In the case of Neural Networks a novel area called TinyML may be useful to improve the computational efficiency of Neural Network models. However, that is out of the scope in this work. Support Vector Machines also require a large computing power as it tries to map data to high dimensional hiperplanes to separate data into the classes. Two models may be mentioned with a moderate computing power requirement, they are: Decision Tree and Random Forest. Decision Tree creates, as its name implies, a decision tree ordered by a feature evaluation function. Usually, the Gini impurity is used as that function identifies the features that best separate the data.\n",
    "\n",
    "The figure below is a good example on how Decision Tree works. \"is the income over or below $30,000?\" is a test performed on the feature called \"Income\" for instance. Each branch represents the outcome of the test, and each leaf node represents a class label. A decision about the class predicted is taken after performing a series of tests on features, until reaching a leaf node. The paths from the root to leaf represent classification rules.\n",
    "\n",
    "<img width=\"450\" height=\"450\" src=\"images for report/decision tree example.png\">\n",
    "\n",
    "The Random Forest model uses various Decision Trees to classify a target. The classification works like an election by using equal or distinct weights for each tree. Each Decision Tree is generated according to a random subset of features. The problem of using this model to Feature Selection task is that some features may not be used in any tree at all. Also, it is harder to find which features best predict the classes as they are randomly selected each time. That is one of the reasons the Decision Tree was chosen as opposed to the Random Forest. More pros of this model can be mentioned:\n",
    "- Lightweight model.\n",
    "- easiness in interpreting the decision tree generated.\n",
    "- The evaluation function is already used for manual Feature Selection.\n",
    "\n",
    "Although, we have to be aware of some cons too:\n",
    "- As it is a simpler model, it may not achieve the expected result.\n",
    "- A large amount of features will require a large tree.\n",
    "\n",
    "In order to evaluate the feature selection generated by the genetic algorithm, the machine learning model has the default hyperparameters [Decision Tree class](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) for all the simulations in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CMz2AAT93WtU"
   },
   "source": [
    "## 4 - Parameters\n",
    "\n",
    "To evaluate the solutions to each modelling problem, we apply various configurations of the following parameters:\n",
    "\n",
    "- **Population**: It is the total number of individuals in each generation. Its value is either 10 or 50 at each generation.\n",
    "\n",
    "\n",
    "- **Iteration limit**: It is the maximum number of iterations. In this problem, the chosen values vary from 100 to 200. \n",
    "\n",
    "\n",
    "- **Stopping criterion**: It is the criterion that stops the genetic algorithm. The first stopping criterion examines the score of individuals (*) and the limit of interactions. The second criterion stops the algorithm once it reaches the maximum iteration limit. \n",
    "\n",
    "\n",
    "- **Crossover type**: Two types of recombination were employed.  The two-point crossover and the crossover according to the percentage of the crossover rate.\n",
    "\n",
    "\n",
    "- **Crossover rate**: It defines the percentage of gene selection of the individual in the crossover. The chosen values are 0.8 and 0.5. \n",
    "\n",
    "\n",
    "- **Mutation type**: There can be two mutation types. Generativemethod, that inverts the values of the genome, and the sequence swap method combined with the generative method.\n",
    "\n",
    "\n",
    "- **Mutation rate**: Percentage of individuals that must be mutated in each generation. The chosen values are 0.03 and 0.15.\n",
    "\n",
    "\n",
    "- **Replicate best individuals**: It is the percentage of the best individuals that are replicated for the next generation. In this case, it can be 10% or 0%. in the last case the entire population is exchanged (exterminio method). \n",
    "\n",
    "\n",
    "\n",
    "(*) The score of individuals is equal to the F1 weighted score. It’s the evaluation metric we use to evaluate each classification model, and thus each individual from the population for all generations. The score varies between 0 and 1. The higher the value, the better the classification model. For more details about the F1 weighted score, please visit this page: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "\n",
    "Also, the Cross Validation strategy with KFold (K=10) is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SSfrxbzkxspB"
   },
   "source": [
    "## 5 - Configurations\n",
    "\n",
    "The GA has an initial configuration, as shown below:\n",
    "\n",
    "**Initial configurations (C0):**\n",
    "\n",
    "- Population: 50.\n",
    "- Maximum number of generations: 100.\n",
    "- Stopping criterion: Maximum number of generations.\n",
    "- Crossover type: Crossover rate selection.\n",
    "- Crossover rate: 80%.\n",
    "- Mutation type: Generative.\n",
    "- Mutation rate: 3%.\n",
    "- Replicate best individuals: 10%.\n",
    "\n",
    "The initial configuration is the basis for the performance evaluation of the GA. Moreover, we modified the parameters of this initial configuration to analyze the impact of these variations on the fitness value and the chromosomes generated at each iteration. Each configuration below corresponds to a change on a single parameter from the initial configuration. For simplicity, we present below only the variation from the initial configuration above:\n",
    "\n",
    "**Configuration 1 (C1):** \n",
    "- Population: 10.\n",
    "\n",
    "**Configuration 2 (C2):** \n",
    "- Maximum number of generations: 200.\n",
    "\n",
    "**Configuration 3 (C3):**\n",
    "- Stopping criterion: Maximum number of generations and fitness value greater than 90%.\n",
    "\n",
    "**Configuration 4 (C4):** \n",
    "- Crossover type: Two crossover points.\n",
    "\n",
    "**Configuration 5 (C5):** \n",
    "- Crossover rate: 50%.\n",
    "\n",
    "**Configuration 6 (C6):**\n",
    "- Mutation type: SeqSwap with generative.\n",
    "\n",
    "**Configuration 7 (C7):**\n",
    "- Mutation rate: 15%.\n",
    "\n",
    "**Configuration 8 (C8):**\n",
    "- Replicate best individuals: 0%. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nGMBvJwpxspC"
   },
   "source": [
    "## 6 -  Metrics\n",
    "\n",
    "The genetic algorithm (GA), which was configured in nine variations, has been executed for a set of datasets to improve the fitness value (the F1 weighted score). To compare the different configurations of parameters of the genetic algorithm, we measure the three following metrics:\n",
    "\n",
    "- **Maximum Fitness Value:** This metric is equal to the fitness value of the chromosome which has the greatest aptitude among the population for a given generation. \n",
    "\n",
    "\n",
    "- **Average Fitness Value:** This metric is the mean value of the fitness value of all the chromosomes inside the population at each generation.  \n",
    "\n",
    "\n",
    "- **Minimum Fitness Value:** This metric is equal to the fitness value of the chromosome, which has the lowest aptitude among the population for a given generation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wZ3sRxABqRwC"
   },
   "source": [
    "# IV - Simulation Results and Discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VgOACYHXxspE"
   },
   "source": [
    "The figures presented in this section show the values derived from a simulation employing the different configurations of the GA algorithm. We compared each configuration in terms of maximum, average, and minimum fitness values. Moreover, the best GA configuration for each dataset is also analized. Looking at the results from all the datasets, we first notice that it takes about 30 generations before the average fitness value of all the individuals stabilizes at a maximum value. This statement is true for all the different configurations. The genetic algorithm thus selects the best combination of features relatively fast. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wZ1ej-b4xI52"
   },
   "source": [
    "## Results overview\n",
    "\n",
    "\n",
    "Before doing a detailed analysis of the genetic algorithm, we briefly show one of the best genomes found by the genetic algorithm. The Bands dataset is used to present the result. In the figure below, the numbers from 0 to 99, on the vertical axis, correspond to the generation number from bottom to top. On the horizontal axis, we list the features in the order they appear in the datasets (from left to right). A dark color corresponds to a feature that was included in the decision tree model by the genetic algorithm, for a given generation.\n",
    "\n",
    "<img src='plots/genomeSequencePlots/bands.csv_50_100_1_0_3_0.8_0_0.15_False_False_0.1.json.png' width=\"450\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m9_FtQtsxspE"
   },
   "source": [
    "## IBM Dataset\n",
    "\n",
    "The fitness values of the best chromosomes and the average fitness value of all chromosomes for each GA configuration employing the IBM dataset are shown in the following figures. Simulation results show that the configuration 5 (C5) produces the highest fitness value among all the configurations. However, the C5 generates a considerable increase in the maximum fitness value when the number of generations is 30. This means that a chromosome, out of average, was generated. Moreover, the C0 and C3 configurations produce highly average fitness values compared to the other configurations. C0 and C3 produce the same behaviors because the stop criterion is the only parameter that was changed, which does not affect the fitness values. Moreover, C6 produces one of the least diverse individuals for all generations, and its average value is just 3% below its maximum value for the best individual.\n",
    "\n",
    "<table><tr><td><img src='plots/IBM_max.png' width=\"450\" height=\"300\"></td><td><img src='plots/IBM_avg.png' width=\"450\" height=\"300\"></td></tr></table>\n",
    "\n",
    "The maximum, average and minimum values for the best configurations (C5) of the GA in the IBM dataset are shown in the following figure. It is possible to verify that even though the average and minimum present a slight increase, the minimum comprehends diverse population, and the average is closer to the maximum. The configuration C5 produces a fitness value of 48 %, which is 20% higher than the lowest fitness value. Moreover, average fitness values are constant starting from generation 40, which means we could have stopped the algorithm earlier and prevented a waste of computation resources. Moreover, a maximum fitness value of 48%, for all configurations, possibly means that the GA drops in a local maximum. This can occur due to the best individuals monopolizing the selection, which is a consequence of an inadequate normalization.\n",
    "\n",
    "<img src=\"plots/IBM_best_config.png\" width=\"450\" height=\"300\">\n",
    "\n",
    "In the next figure, we present the chromosomes generated. On the left figure, we present the best chromosome at each generation with the C5 configuration. On the right figure, we show the least \"diverse\" best genome sequence with the C6 configuration. It is possible to observe that even with less features in the left result, it is better than the right one. That said,  in featuring selection to select the right features is better than to remove only to decrease the computational effort of a model.\n",
    "\n",
    "<table><tr><td><img src='plots/bestgenomePlots/IBM.csv_50_100_1_0_3_0.5_0_0.03_False_False_0.1.json.png' width=\"450\" height=\"300\"></td><td><img src='plots/bestgenomePlots/IBM.csv_50_100_1_0_3_0.8_1_0.03_False_False_0.1.json.png' width=\"450\" height=\"300\"></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uTNFuKWzwsh0"
   },
   "source": [
    " ##  Bands Dataset\n",
    "\n",
    "The fitness values of the best chromosomes and the average fitness value of all chromosomes for each GA configuration employing the bands dataset are shown in the following figures. Simulation results show that the C3 configuration has the highest Fitness within 100 iterations. However, the C2 has a limit of 200 iterations and exceeds the Fitness values of all configurations after 175 iterations. Moreover, the C0 and C6 configurations obtained Fitness values lower than the other configurations. This is because the C0 has a smaller population, just 10 individuals, and the C6 configuration uses sequential swap mutation with generation, which causes less diversity between individuals.\n",
    "\n",
    "<table><tr><td><img src='plots/bands_max.png' width=\"450\" height=\"300\"></td><td><img src='plots/bands_avg.png' width=\"450\" height=\"300\"></td></tr></table>\n",
    "\n",
    "The maximum, average and minimum values for the best configurations (C2) of the GA in the Bands dataset are shown in the following figure. Considering the C2 configuration, the maximum Fitness value of Bands dataset is 0.85, and the minimum is 0.64, which represent 85% and 64%, respectively. Consequently, it comprehends a substantial improvement of 21% in the result. The average values begin to stabilize after 50 iterations which implies that individuals with poor results does not affect harshly the overral population result.\n",
    "\n",
    "<img src=\"plots/bands_best_config.png\" width=\"450\" height=\"300\">\n",
    "\n",
    "\n",
    "On the left figure above, we present the best genome with the C3 configuration. On the right figure, we show the best genome sequence with the C2 configuration, containing 200 generations. The strength of the color blue dictates how diverse are the chromosomes generated, as darker the color less diverse the best individual of the population is. Consequently, C3 obtained less diverse best individual.\n",
    "\n",
    "<table><tr><td><img src='plots/bestgenomePlots/bands.csv_50_100_0_0_3_0.8_0_0.03_False_False_0.1.json.png' width=\"450\" height=\"300\"></td><td><img src='plots/bestgenomePlots/bands.csv_50_200_1_0_3_0.8_0_0.03_False_False_0.1.json.png' width=\"450\" height=\"300\"></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vyvaIMj8wsh1"
   },
   "source": [
    " ##  Flag Dataset \n",
    "\n",
    "The fitness values of the best chromosomes and the average fitness value of all chromosomes for each GA configuration employing the flag dataset are shown in the following figures. Simulation results show that the configuration 6 (C2) produces the highest fitness value compared to the other configurations. However, this fitness value is slightly higher than those produced by the other configurations, in the best case, c6 produces a fitness 3% higher than the worst configuration (C8). Thus, all configurations produce almost the same results. However, the configuration C8 produces the lowest average fitness values since the new population is created by extermination method, which increases the variability of the best chromosome.\n",
    "\n",
    "<table><tr><td><img src='plots/flag_max.png' width=\"450\" height=\"300\"></td><td><img src='plots/flag_avg.png' width=\"450\" height=\"300\"></td></tr></table>\n",
    "\n",
    "The maximum, average and minimum values for the best configurations (C2) of the GA in the bands dataset are shown in the following figure. The C2 configuration produces a high diversity in the chromosomes, which is observed in the chromosome with the least fitness value. The fitness values of these chromosomes vary from 30% to 70%  in all generations.  Moreover, the average fitness value converges after 15 generations. Also, the fitness value of the best chromosome has the highest increase until that generation. After generation 15, the increase in the value of fitness is approximately 3%. The average is really close to the best result, which implies that it is easy to improve a model by removing some features of this dataset. Almost at the end there is a considerable decrease in the worst individual, we may see that behavior in the average even though in a distinct proportion.\n",
    "\n",
    "<img src=\"plots/flag_best_b_a_m.png\" width=\"450\" height=\"300\">\n",
    "\n",
    "On the figure below, we show the best genome sequence with the C2 configuration, containing 200 generations. It is possible to visualize that 9 features are almost never in the best individual's chromosome. That may be used to decrease the quantity of iterations in a future work so that this features are not considered.\n",
    "\n",
    "<img src='plots/bestgenomePlots/flag.csv_50_200_1_0_3_0.8_0_0.03_False_False_0.1.json.png' width=\"450\" height=\"300\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NL1Wja6wwsh2"
   },
   "source": [
    " ##  Cellphone Dataset \n",
    "\n",
    "\n",
    "The fitness values of the best chromosomes and the average fitness value of all chromosomes for each GA configuration employing the Cellphone dataset are shown in the following figures. It is interesting to note that in the Cellphone dataset, the maximum values of Fitness for all configurations were very similar. Moreover, the average of the C6 and C7 configurations was lower than the average of the others. However, they all stabilized after around 20 iterations. Also, the C3 configuration showed the best behavior.\n",
    "\n",
    "<table><tr><td><img src='plots/cellphone_max.png' width=\"450\" height=\"300\"></td><td><img src='plots/cellphone_avg.png' width=\"450\" height=\"300\"></td></tr></table>\n",
    "\n",
    "The maximum, average and minimum values for the best configuration (C3) of the GA in the bands dataset are shown in the following figure. The C3 configuration produces the highest Fitness value using the stop criterion Maximum Number of Generations and fitness value greater to 90%. In this configuration, the fitness values of minimum chromosomes vary from 30% to 60%, without stabilization. Despite that, the average fitness values start to stabilize after 10 iterations.\n",
    "\n",
    "<img src=\"plots/cellphone_best_config.png\" width=\"450\" height=\"300\">\n",
    "\n",
    "The figure below presents the best chromosome with the C3 configuration at each generation. The same observation done in the last dataset may be done here. There are 10 features that almost never appear in the best individual which may be removed by the dataset to improve the result.\n",
    "\n",
    "<img src=\"plots/bestgenomePlots/cellphone.csv_50_100_0_0_3_0.8_0_0.03_False_False_0.1.json.png\" width=\"450\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4YcURaEZwsh3"
   },
   "source": [
    " ##  Mushrooms Dataset \n",
    "\n",
    "The fitness values of the best chromosomes and the average fitness value of all chromosomes for each GA configuration employing the mushrooms dataset are shown in the following figures. The GA, in all configurations, produces the maximum fitness value after a few generations, which means the dataset has a low complexity. Moreover, the stop criteria employed in the configuration generated a high waste of computational resources since the genetic algorithm could have stopped after a few iterations.\n",
    "\n",
    "<table><tr><td><img src='plots/mushrooms_max.png' width=\"450\" height=\"300\"></td><td><img src='plots/mushrooms_avg.png' width=\"450\" height=\"300\"></td></tr></table>\n",
    "\n",
    "The figure below presents the best chromosome with the C3 configuration at each generation. What may be observed is that even though the final result is 100% almost at the start of the algorithm, some features may be removed without impacting the final result. \n",
    "\n",
    "<img src=\"plots/bestgenomePlots/mushrooms.csv_50_100_0_0_3_0.8_0_0.03_False_False_0.1.json.png\" width=\"450\" height=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lOM5WOFp0W8p"
   },
   "source": [
    "## Airline Customer Satisfaction Dataset\n",
    "\n",
    "\n",
    "The fitness values of the best chromosomes and the average fitness value of all chromosomes for each GA configuration employing the Airline Customer Satisfaction dataset are shown in the following figures. The genetic algorithm produced relatively good results on the dataset “Airline Customer Satisfaction” in general for all the combinations in parameters.  Looking at the figure above titled “Airline Customer Satisfaction - Maximum”, most of the scenarios generate a “best” chromosome with a fitness value a little above 0.95 from generation 40 and after. Scenario C3 converged the fastest to the optimal chromosome, from generation 20, to give the highest fitness value. Scenario C2 produces the “least good” chromosomes, at each generation, with a final fitness value just above 94%. We also note here the impact of randomness on the results. In fact, C0 has the same strategy as C3, the differences in the fitness values having nothing to do in the creation of new generations, but to do with the randomness in the mutation and crossover processes. Except for C2 and C6, all the other configurations reach a fitness value above 94% given by their best chromosomes.\n",
    "\n",
    "<table><tr><td><img src='plots/Metrics2/airline_customer_satisfaction_bestIndividuals.png' width=\"450\" height=\"300\"></td><td><img src='plots/airline_customer_satisfaction_avg.png' width=\"450\" height=\"300\"></td></tr></table>\n",
    "\n",
    "On the left figure below, we present the best genome with the C4 configuration. On the right figure, we show the best genome sequence with the C2 configuration, giving the least good fitness value. It is possible to visualize that just two features separate the best genome sequences from C4 and C2 (\"Leg room service\" and \"cleaniness\"). That difference results in almost a 1% difference in the f1-score obtained.\n",
    "\n",
    "<table><tr><td><img src='plots/bestgenomePlots/airline_customer_satisfaction.csv_50_100_0_0_3_0.8_0_0.03_False_False_0.1.json.png' width=\"450\" height=\"300\"></td><td><img src='plots/bestgenomePlots/airline_customer_satisfaction.csv_50_100_1_0_3_0.8_0_0.15_False_False_0.1.json.png' width=\"450\" height=\"300\"></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fAqmXaHdzB_7"
   },
   "source": [
    "## Result summarization according to the F1-score\n",
    "\n",
    "In this section, we compare the results for each dataset without selection and with selection (according to each distinct configuration). We define “without selection” as building the decision tree model with all the features present in the datasets. The figure below shows the histograms of fitness values “without selection” against the results discussed earlier. For the “IBM” dataset, removing some features decreases the quality of the model. For the \"Airline Customer Satisfaction\" problem, feeding all the features or just the selected features from the GA makes a very small difference (at most 1% of difference in the F1 score). However, for all the other datasets, we notice that feeding all the features to the decision tree model isn’t a good idea. In fact, feature selection provided by the genetic algorithm improved the models’ score from 1% to more than 10%. That underlines the capability of the strategy to select the best features from the datasets to classify the target. Even if only a slight improvement was gained, notably for the dataset “cellphone”, removing some features from the datasets has some benefits on many levels. It decreases the size of the datasets in terms of features and observations, and reduces the processing and memory requirements to train and use the model. It also decreases the response time of the model when deployed in production.\n",
    "\n",
    "\n",
    "<img src=\"plots/res_classification.png\" width=\"800\" height=\"300\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In the table below, we show the numbers of features finally selected by the genetic algorithm that give the best decision tree model for each classification problem.\n",
    "\n",
    "Dataset name | Number of features | Number of selected features with GA | Features selected as % of all features \n",
    "--------- | --------- | --------- | --------- |\n",
    "Glass | 9 |  5 |  55%\n",
    "Cellphone | 20  | 10 |  50%\n",
    "Mushrooms | 22  | 10 |  46%\n",
    "Airline Customer Satisfaction | 22  | 7 |  32%\n",
    "Kobe | 24 | 9 |  38%\n",
    "Flag | 29  | 16 |  55%\n",
    "IBM | 34 | 17 |  50%\n",
    "Band | 37 | 23 |  62%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iN3x766D-kH1"
   },
   "source": [
    "## All the other datasets\n",
    "\n",
    "At the time of writing of this report, the results of the feature selection provided by the genetic algorithm for the other datasets, listed in the table above, weren't yet available for analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z5OncdpEqRwC"
   },
   "source": [
    "## Discussions\n",
    "\n",
    "**Time of execution of the GA per number of features**\n",
    "\n",
    "For a relatively high number of features (50 and above), the execution time of the genetic algorithm increases very fast. For this reason, it wasn’t possible to show the results for the Human Activity Classification dataset, its total number of features being above 500. We should also add that the decision tree model is relatively fast to train. Combining the genetic algorithm with other artificial intelligence models such as neural networks would require simulation time of much higher magnitude.\n",
    "\n",
    "**Importance on the choice of configurations**\n",
    "\n",
    "As we saw during the analysis of the results, different configurations produce different results in terms of fitness values. Thus, it’s not possible to select which scenario specifically is the best one for all the datasets. There isn’t a “one size fits all” scenario. In addition, due to the randomness of the mutation and crossover processes, we recommend running the genetic algorithm a couple of times in order to get the best feature selection in terms of fitness value. \n",
    "\n",
    "**Limited results for some datasets**\n",
    "\n",
    "Looking at the plots for all the datasets, the results are quite limited for some classification problems (the “Kobe” dataset for instance).  It’s important to point out that the limited results aren’t due to the genetic algorithm but related to the choice of the decision tree model or to the choice of features selected. Therefore, the strategy in this case might be to explore other possible features that might impact in some ways the target to classify. If that option was already explored then we can conclude that the decision tree classification model might not be the best machine learning technique for the problem at hand. Thus, they can save time and try a more complex classification technique such as k-clustering, SVM, or neural networks for example. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2eiLiZKJqRwD"
   },
   "source": [
    "# V - Conclusions\n",
    "\n",
    "As we can see from this project, combining a machine learning method with  the genetic algorithm in order to find the best combinations of features, from all the features available in a dataset, can be an interesting strategy. \n",
    "The genetic algorithm is very efficient at selecting, combining and trying various subsets of features in order to improve the fitness value and build the best classification model.\n",
    "\n",
    "For the vast majority of the datasets in our project, simply feeding all the features to the decision tree model produces limited results. The genetic algorithm is able to select the most appropriate features with the end goal of improving the fitness value of the machine learning model. In our case, the fitness value is the F1 score, but the algorithm can be easily adapted to cater to any other fitness function, or to another modeling problem such as regression. \n",
    "\n",
    "Moreover, due to \"the curse of dimensionality\", the more we include features in the datasets, the more we need observations for model building and the higher the dataset size in terms of memory size. This has a significant impact on the execution time for the genetic algorithm. However, we should temper that fact by remembering that we need to execute the genetic algorithm only once for feature selection. On another note, we could improve the stop criterion of the genetic algorithm to make it stop earlier when, after a reasonable amount of iterations, we see no improvement in the fitness value.\n",
    "\n",
    "In addition, the decision tree model is relatively easy and fast to build. If someone isn't sure which machine learning model to use, then the algorithm of this project might give them an answer whether the decision tree model is appropriate to the datasets and the problem at hand. If the fitness values found are limited, then either the features selected aren't adequate, and someone might want to collect other features, or they might select a more complex machine learning model.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CE35siLs3ICj"
   },
   "source": [
    "# VI - Sources\n",
    "\n",
    "***Datasets used in the project:***\n",
    "\n",
    "Airline Customer Satisfaction: https://www.kaggle.com/teejmahal20/airline-passenger-satisfaction\n",
    "\n",
    "Kobe: https://www.kaggle.com/c/kobe-bryant-shot-selection/data\n",
    "\n",
    "IBM: https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset\n",
    "\n",
    "Human Activity Classification: https://www.kaggle.com/uciml/human-activity-recognition-with-smartphones?select=train.csv\n",
    "\n",
    "Flag: https://archive.ics.uci.edu/ml/datasets/Flags\n",
    "\n",
    "Mushrooms: https://www.kaggle.com/uciml/mushroom-classification\n",
    "\n",
    "Glass: https://www.kaggle.com/uciml/glass\n",
    "\n",
    "Bands: https://archive.ics.uci.edu/ml/datasets/Cylinder+Bands\n",
    "\n",
    "Cellphones: https://www.kaggle.com/iabhishekofficial/mobile-price-classification\n",
    "\n",
    "\n",
    "***Link to plots:***\n",
    "\n",
    "Plots of the fitness values in function of the configuarion method and generation: https://github.com/lucaslzl/ga_ia_p2/tree/master/plots\n",
    "\n",
    "Plots of the best genome sequences in function of the configuration and generation: https://github.com/lucaslzl/ga_ia_p2/tree/master/plots/bestgenomePlots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aTbhA9brxspH"
   },
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WOYQDIT_xspI"
   },
   "source": [
    "## **Link to the Github repository**\n",
    "\n",
    "[https://github.com/lucaslzl/ga_ia_p2](https://github.com/lucaslzl/ga_ia_p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cyi50hfFxspI"
   },
   "source": [
    "## **Link to the video**\n",
    "\n",
    "https://www.youtube.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vyvsgQwexspJ"
   },
   "source": [
    "## **How to execute the files**\n",
    "\n",
    "There are two ways to execute the experiments:\n",
    "\n",
    "**1. Execute the python code called \"main.py\" and pass the parameters**\n",
    "\n",
    "The code considers the parameters to execute each strategy and map. It is possible to change what is the stop criteria, crossover type and more.\n",
    "\n",
    "Run \"python3 main.py --(strategy flag)\" to execute.\n",
    "\n",
    "For instance, \"python main.py --population=50 --dataset=path --iteration_limit=100 --stop_criteria=1 --probs_type=0 \n",
    "--crossover_type=3 --crossover_rate=0.8 --mutation_type=0 --mutation_rate=0.03 --use_threads=0 --cut_half_pop=0\n",
    "--replicate_best=0.1\" executes a single feature selection with the mentioned parameters.\n",
    "\n",
    "It is worth to point out that you may run \"python main.py --help\" to visualize all the possible flags and parameters.\n",
    "\n",
    "**2. Execute the shellcode called \"execute.sh\"**\n",
    "\n",
    "The code executes every configuration for each dataset and saves the results.\n",
    "\n",
    "run \"./execute.sh\" and wait until it is done.\n",
    "\n",
    "All results are saved in the results folder.\n",
    "\n",
    "Good idea to use virtual env. Tested on Python 3.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7WEKqmU2wsh9"
   },
   "source": [
    "## Individual Contributions\n",
    "\n",
    "**Planning**\n",
    "- **Genetic Algorithm**: Matheus Ferraroni\n",
    "- **Methodology**: Entire Group\n",
    "- **Experiments**: Oscar Ciceri, Maria Vitória R. Oliveira\n",
    "\n",
    "**Code**\n",
    "- **Genetic Algorithm**: Matheus Ferraroni Sanches, Oscar Ciceri\n",
    "- **Machine Learning**: Lucas Zanco Ladeira, Aissa Hadj\n",
    "- **Plots**: Maria Vitória R. Oliveira, Aissa Hadj\n",
    "\n",
    "**Report**\n",
    "- **I Introduction**: Lucas Zanco Ladeira\n",
    "- **II Genetic Algorithm**: Matheus Ferraroni Sanches\n",
    "- **III Methodology**: Lucas Zanco Ladeira, Aissa Hadj, Oscar Ciceri, Maria Vitória R. Oliveira\n",
    "- **IV Simulation Results**: Maria Vitória R. Oliveira, Oscar Ciceri, Aissa Hadj, Lucas Zanco Ladeira \n",
    "- **V Conclusions**: Aissa Hadj\n",
    "- **VI Sources**: Lucas Zanco Ladeira, Aissa Hadj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Vsyf19A0q2sQ"
   ],
   "name": "report.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
