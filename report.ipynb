{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uDgcJ3xfSwxh"
   },
   "source": [
    "# **Introduction to Artificial Intelligence - MO416A**\n",
    "**UNIVERSITY OF CAMPINAS**\n",
    "\n",
    "\n",
    "\n",
    "This work was completed by the following members:\n",
    "\n",
    "\n",
    "\n",
    "*   Aissa Hadj - 265189\n",
    "*   Lucas Zanco Ladeira - 188951\n",
    "*   Matheus Ferraroni - 212142\n",
    "*   Maria Vit√≥ria Rodrigues Oliveira - 262884\n",
    "*   Oscar Ciceri - 164786\n",
    "\n",
    "The original code of the project is located on a [repository inside Github](https://github.com/lucaslzl/ga_ia_p2) and the video showing the search strategies working is on [youtube](https://youtube.com). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vsyf19A0q2sQ"
   },
   "source": [
    "# 1 - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H8PUV6MgqRvZ"
   },
   "source": [
    "\n",
    "\n",
    "The problem that will be tackled in this project is Feature Selection. The goal is to obtain the subset of available features in a dataset that improves model performance by increasing its accuracy and decreasing its error rate. With the presence of irrelevant features in the dataset, more processing and memory requirements are necessary, thus wasting computing resources. To better understand the possible impacts of feature selection, we could cite the following pros: \n",
    "\n",
    "- Reducing Overfitting (nao concordo com esse ponto)\n",
    "- Improving the model Accuracy\n",
    "- Reducing Training Time\n",
    "- removing some features with redundant information\n",
    "- saving time and costs by collecting fewer features when deploying the model in production\n",
    "- requiring less computing resources such as memory\n",
    "- reducing the delay of the model in generating results\n",
    "\n",
    "Feature Selection may be done manually or automatically. Some manual techniques include: univariate selection, feature importance, and the correlation matrix. The objective of the Univariate selection method is to statistically describe the relation between each feature and the target. Also, feature importance generates a score for each feature in order to rank it. For instance, Decision Tree algorithms may rank features according to Gini impurity tests. Finally, the Correlation Matrix shows the correlation between pairs of features so that the features with a high correlation could be removed. The literature presents the usage of optimization techniques to automatically find the best (or a quite good) subset of features. Some of the methods include:\n",
    "\n",
    "- <b>Exhaustive search</b>\n",
    "- <b>Simulated Annealing</b>\n",
    "- <b>Transformation Graph</b>\n",
    "- <b>Genetic Algorithms</b>\n",
    "\n",
    "<b>Exhaustive Search</b> is not an optimization technique, but it is worth to be mentioned as its computational complexity is $O(2^n)$. This technique tries every possible subset of features in order to find the best one. Due to its computational complexity, this technique is not practical in most cases. <b>Simulated Annealing</b> is a metaheuristic for complex nonlinear optimization problems and is analogous to the simulation of the annealing of solids. The analogy pairs are as follows: feasible solution (state), cost (energy), optimal solution (ground state), local search (rapid quenching), simulated annealing (careful annealing). On the other hand, <b>Transformation Graph</b> is a strategy that utilizes a tree-like structure to generate possible solutions. First, $n$ solutions are generated by removing at each one distinct feature. Second, all the solutions are evaluated. Third, the best solution is chosen and $n-1$ are generated by removing each feature yet not removed. That strategy goes on considering a budget. The issue of this strategy is that it requires a substantial amount of memory. Finally, <b>Genetic Algorithm</b> is inspired by genetics (DNA) to search through solutions. Its process can be described with the following steps: (1) it generates a population considering variations of the DNA, (2) then ranks the population according to some score, it applies some forms of mutations and other transformations to the DNA at each generation, and finally, (3) it iterates through the previous steps until it reaches a stopping rule. The stopping rule may be the amount of generations produced or a target solution was reached. In the current project, we decided to further explore this strategy by applying it to a specific modelization problem. More details will be given in the next sections.\n",
    "\n",
    "This report is structured as follows. In section 2, we describe the implementation of the main parts of the genetic algorithm. Then, in section 3, we discuss the methodology we followed to undertake this project. Finally, in section 4, we do a detailed analysis of the results.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sW3ewDHPqRvc"
   },
   "source": [
    "# 2 - Genetic Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZPkoG23yqRvd"
   },
   "source": [
    "We developed 2 classes to control the operation of the genetic algorithm. The classes are 'element' and 'GeneticAlgorithm' and can be seen in https://github.com/lucaslzl/ga_ia_p2/blob/master/GA.py. In this notebook, the class 'element' will be explained, as well as the main methods of the class 'GeneticAlgorithm'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YEgj38W2qRvd"
   },
   "source": [
    "Below, the entire class 'element' is presented. It is possible to see that this class is only responsible for managing the id, the generation, the genome and the score of each element of the population for every generation. Saving this attributes in the same place can be useful for different approaches during the implementation and use of the methods on the genetic algorithm. It is also possible to save the parents of each element and traceback how each element was formed during the evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JYT_fL3wqRve"
   },
   "outputs": [],
   "source": [
    "class element:\n",
    "\n",
    "    def __init__(self, idd, geracao, genome):\n",
    "        self.idd = idd\n",
    "        self.geracao = geracao\n",
    "        self.genome = genome\n",
    "        self.score = None\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"(id=\"+str(self.idd)+\",geracao=\"+str(self.geracao)+\",score=\"+str(self.score)+\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AwQyOY1wqRvk"
   },
   "source": [
    "The genetic algorithm implemented is very generic, this means that it can be applied easily to different problems. Using this kind of generic implementation makes it practical to override the functions responsible to generate a random genome, to mutate and perform the fitness calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ncTpcU_SqRvl"
   },
   "source": [
    "The method 'create_initial_population' is called once the genetic algorithm is launched. This method is responsible for creating the individuals of the initial population and add them to the pool until the required population size is reached. To create the genome, the function 'random_genome', that was overwritten before, is called for each element.\n",
    "\n",
    "In our problem, we define the genome as an array of 0's and 1's as elements with a length equal to the amount of features in the dataset. This kind of approach used in the genome allows us to decode the genome as:\n",
    "\n",
    "- Every bit of the genome corresponds to a feature in the dataset\n",
    "- If the bit is 1: the feature corresponding to that bit is used\n",
    "- If the bit is 0: the feature corresponding to that bit isn't used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fx3_fIfgqRvl"
   },
   "outputs": [],
   "source": [
    "def create_initial_population(self):\n",
    "    for _ in range(self.population_size):\n",
    "        self.population.append(element(self.elements_created, 0, self.random_genome()))\n",
    "        self.elements_created += 1\n",
    "\n",
    "\n",
    "def random_genome():\n",
    "    return np.random.randint(low=0,high=2,size=len(df.columns),dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yE4dvXf_dEqo"
   },
   "source": [
    "The method 'run' is where the main loop of the genetic algorithm is executed. The steps are:\n",
    "\n",
    "1. Check the stop criteria\n",
    "2. Calculate the score for the current population\n",
    "3. Sort the population according to the fitness value\n",
    "4. Update the solution if a better result was found\n",
    "5. Save the log\n",
    "6. If set, part of the worst part the of population can be discarted (This is not being used in the solutions found for this work)\n",
    "7. Create a new population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_mIDkEOhqRvp"
   },
   "outputs": [],
   "source": [
    "def run(self):\n",
    "\n",
    "    while self.check_stop():\n",
    "        self.calculate_score() \n",
    "        self.population.sort(key=lambda x: x.score, reverse=True) \n",
    "\n",
    "        if self.best_element_total==None or self.population[0].score > self.best_element_total.score: \n",
    "            self.best_element_total = self.population[0]\n",
    "\n",
    "        self.do_log()\n",
    "\n",
    "        if self.cut_half_population: \n",
    "            self.population = self.population[0:len(self.population)//2] \n",
    "\n",
    "        self.new_population()\n",
    "\n",
    "        self.iteration_counter +=1\n",
    "\n",
    "    return self.best_element_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GhQ2dBOcdEqt"
   },
   "source": [
    "The creation of a new population is implemented to be the same independently to how the selection of the parents is made. In order to achieve this, we create an array of probability that implements the rules for this selection based on equal chance of each element or by roulette, where elements with higher fitness have higher chances to be selected.\n",
    "\n",
    "The crossover method receives the parent's genome and returns a new genome based on its genome.\n",
    "This new genome is inserted into the new element and this new element is added to the new pool.\n",
    "\n",
    "If we are recreating the entire population, this proccess repeats till the new population has the size of the population size limit. If we are replicating a percentage of the best elements, the amount of best elements being replicated is reduced in this proccess and they are replicated after this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C0KNu6B_qRvs"
   },
   "outputs": [],
   "source": [
    "def new_population(self):\n",
    "\n",
    "    probs = self.get_probs()\n",
    "    newPop = []\n",
    "    best_replicator = int(self.population_size*self.replicate_best)\n",
    "\n",
    "    while len(newPop)<self.population_size-best_replicator:\n",
    "        parents = np.random.choice(self.population,size=2,p=probs) \n",
    "\n",
    "        if parents[0].score<parents[1].score: \n",
    "            parents = parents[::-1] \n",
    "\n",
    "        new_element = element(self.elements_created, self.iteration_counter, self.crossover(parents[0].genome, parents[1].genome))\n",
    "\n",
    "        new_element.genome = self.active_mutate(new_element.genome)\n",
    "        newPop.append(new_element)\n",
    "        self.elements_created += 1\n",
    "\n",
    "    for i in range(best_replicator):\n",
    "        newPop.append(self.population[i])\n",
    "\n",
    "    self.population = newPop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JyMBfzT1dEq1"
   },
   "source": [
    "In order to define the probability of being selected as a parent, we implement 3 methods: \"get_probs\", \"probs_equal\" and \"probs_roulette\".\n",
    "\n",
    "The \"get_probs\" just checks what kind of probability function must be used and calls the right one.\n",
    "The \"probs_equal\" returns an array of probability where every element has the same chance of being selected.\n",
    "The \"probs_roullete\" adds the fitness of each element to the array, sums their total and divides the array by this sum. The result is an array where the best solutions have higher chances of being selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xrkGFw9GqRvw"
   },
   "outputs": [],
   "source": [
    "def get_probs(self):\n",
    "    if self.probs_type == 0:\n",
    "        return self.probs_roulette()\n",
    "    elif self.probs_type == 1:\n",
    "        return self.probs_equal()\n",
    "\n",
    "\n",
    "def probs_equal(self):\n",
    "    return [1/len(self.population)]*len(self.population)\n",
    "\n",
    "\n",
    "def probs_roulette(self):\n",
    "    probs = [0]*len(self.population) \n",
    "    for i in range(len(probs)):\n",
    "        probs[i] = self.population[i].score\n",
    "    div = sum(probs)\n",
    "\n",
    "    if div!=0:\n",
    "        for i in range(len(probs)):\n",
    "            probs[i] /= div\n",
    "    else: \n",
    "        probs = self.probs_equal()\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kEPu0GC7dEq6"
   },
   "source": [
    "The method \"check_stop\" is responsible for checking which stop criteria must be used. The stop criteria can be used in 3 different ways.\n",
    "\n",
    "The \"stop_criteria_iteration\" just returns \"True\" if a minimum amount of iterations has been attained.\n",
    "The \"stop_criteria_score\" just returns \"True\" if any solution for a given generation achieved a minimum score.\n",
    "The \"stop_criteria_double\" is a mix of the previous 2 methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I3zwbZjtqRvz"
   },
   "outputs": [],
   "source": [
    "def check_stop(self):\n",
    "    if self.stop_criteria_type==0:\n",
    "        return self.stop_criteria_double()\n",
    "    elif self.stop_criteria_type==1:\n",
    "        return self.stop_criteria_iteration()\n",
    "    elif self.stop_criteria_type==2:\n",
    "        return self.stop_criteria_score()\n",
    "\n",
    "def stop_criteria_double(self):\n",
    "    s = self.population[0].score\n",
    "    if s==None:\n",
    "        s = 0\n",
    "    return self.iteration_counter<self.iteration_limit or s>=self.max_possible_score\n",
    "\n",
    "def stop_criteria_iteration(self):\n",
    "    return self.iteration_counter<self.iteration_limit\n",
    "\n",
    "def stop_criteria_score(self):\n",
    "    s = self.population[0].score\n",
    "    if s==None:\n",
    "        s = 0\n",
    "    return s>=self.max_possible_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p4V5JOLddEq-"
   },
   "source": [
    "We implemented 4 different crossovers and 1 function to decide which one must be used. The function \"crossover\" just receives the genome of 2 parents and calls the right genome function.\n",
    "\n",
    "The \"crossover_rate_selection\" iterates the genome of both parents and selects from which one the bit must be selected. In order to define which one to select, this function checks a percentage that was defined previously. This means that the resulting genome can be 80% from one parent and 20% from the other, where the 80% comes from a parent with the highest score.\n",
    "\n",
    "The \"crossover_uniform\" just selects the bits from the parent with a chance close to 50%/50% from selection from each parent.\n",
    "\n",
    "The \"crossover_single_point\" defines a random point in the middle of the genome and picks the first part from parentA and the second part from the parentB.\n",
    "\n",
    "The \"crossover_two_point\" defines 2 random points in the middle of the genome and cocatenates a part of the genomeA, a part from genomeB and a part from the genomeA using the points defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0eAPc2T5qRv3"
   },
   "outputs": [],
   "source": [
    "def crossover(self, genA, genB):\n",
    "    if self.crossover_type==0:\n",
    "        return self.crossover_uniform(genA, genB)\n",
    "    elif self.crossover_type==1:\n",
    "        return self.crossover_single_point(genA, genB)\n",
    "    elif self.crossover_type==2:\n",
    "        return self.crossover_two_point(genA, genB)\n",
    "    elif self.crossover_type==3:\n",
    "        return self.crossover_rate_selection(genA, genB)\n",
    "\n",
    "def crossover_rate_selection(self, genA, genB):\n",
    "    new = np.array([],dtype=int)\n",
    "    for i in range(len(genA)):\n",
    "        if np.random.random()<self.crossover_rate:\n",
    "            new = np.append(new, genA[i])\n",
    "        else:\n",
    "            new = np.append(new, genB[i])\n",
    "    return new\n",
    "\n",
    "\n",
    "def crossover_uniform(self, genA, genB):\n",
    "    new = np.array([],dtype=int)\n",
    "    for i in range(len(genA)):\n",
    "        if np.random.random()<0.5:\n",
    "            new = np.append(new, genA[i])\n",
    "        else:\n",
    "            new = np.append(new, genB[i])\n",
    "    return new\n",
    "\n",
    "\n",
    "def crossover_single_point(self, genA, genB):\n",
    "    p = np.random.randint(low=1,high=len(genA)-1) \n",
    "    return np.append(genA[0:p],genB[p:])\n",
    "\n",
    "\n",
    "def crossover_two_point(self, genA, genB):\n",
    "    c1 = c2 = np.random.randint(low=0,high=len(genA)) \n",
    "    while c2==c1: \n",
    "        c2 = np.random.randint(low=0,high=len(genA))\n",
    "\n",
    "    if c1>c2: \n",
    "        c1, c2 = c2,c1\n",
    "\n",
    "    new = np.append(np.append(genA[0:c1],genB[c1:c2]),genA[c2:]) \n",
    "    return new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SaY4R1d4dErC"
   },
   "source": [
    "The method \"calculate_score\" was implemented to be used synchronously or asynchronously and the user can define how to execute it before the start of the main loop.\n",
    "\n",
    "This function just passes the genome of each element to a function that returns its fitness. In our case, we call the function \"evaluate\" that is responsible to decode the genome to use or not the features in the dataset and to check how good these features perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aZ6K_G3NqRv6"
   },
   "outputs": [],
   "source": [
    "def calculate_score(self):\n",
    "    if self.use_threads: \n",
    "\n",
    "        threads_running = []\n",
    "        for e in self.population:\n",
    "            x = threading.Thread(target=self.thread_evaluate, args=(e,))\n",
    "            x.start()\n",
    "            threads_running.append(x)\n",
    "\n",
    "        for i in range(len(threads_running)):\n",
    "            threads_running[i].join()\n",
    "\n",
    "    else: \n",
    "        for e in self.population:\n",
    "            e.score = self.evaluate(e.genome)\n",
    "\n",
    "def thread_evaluate(self, e):\n",
    "    e.score = self.evaluate(e.genome)\n",
    "    \n",
    "\n",
    "def evaluate(genome):\n",
    "    bool_genome = list(map(bool, genome))\n",
    "    return model.evaluate(df.loc[:, bool_genome].copy(), target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Sy4V2b8dErF"
   },
   "source": [
    "The mutation method is called \"active_mutate\" and receives a single genome. This method iterates through the entire genome and creates a random value, if this value is smaller than the one set in the initialization, then a mutation is started on that index.\n",
    "\n",
    "We implemented 2 different mutations for this project:\n",
    "\n",
    "- mutate1: This method implements the generative mutation, which randomly changes a gene.  The genes have binary values; thus, the selected gene changes the allele value for his complement.\n",
    "\n",
    "\n",
    "- mutate2: This method implements the sequence swap combined with a generative mutation. First, a random position of the gene on a chromosome is selected. The genes located after this position are move to the beginning on the chromosome, and genes located before are move to the last. Moreover, the generative mutations technique is employed in the new chromosome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DXlHRcy5qRv-"
   },
   "outputs": [],
   "source": [
    "def active_mutate(self,gen):\n",
    "    if self.mutation_rate<=0: \n",
    "        return gen\n",
    "    for i in range(len(gen)): \n",
    "        if np.random.random()<self.mutation_rate: \n",
    "            gen = self.mutate1(i, gen) \n",
    "    return gen\n",
    "\n",
    "\n",
    "def mutate1(index, genome):\n",
    "    if genome[index]==0:\n",
    "        genome[index] = 1\n",
    "    else:\n",
    "        genome[index] = 0\n",
    "    return genome\n",
    "\n",
    "\n",
    "def mutate2(index, genome):\n",
    "    aux = []\n",
    "    for i in range(len(genome)):\n",
    "        if i <= index:\n",
    "            aux.append(genome[i])\n",
    "        else:\n",
    "            aux.insert(0, genome[i])\n",
    "    genome = aux\n",
    "    if genome[index]==0:\n",
    "        genome[index] = 1\n",
    "    else:\n",
    "        genome[index] = 0\n",
    "    return genome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o8OBSNg3qRwA"
   },
   "source": [
    "# 3 - Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kgovD14lqRwB"
   },
   "source": [
    "In this project, we apply the genetic algorithm to the frequent challenge of building machine learning models applied to classification problems. The term \"target\" is defined as the variable that the machine learning model must predict and classify as one class among a choice of k classes. In order to build the model, we use \"features\" that are defined as the modalities that are observed with the assumption that they are relevant in making the prediction about the target. The objective in our work is thus to measure the importance of each feature in helping to derive the value of the target, relative to the other available features. The end goal is thus to perform a feature selection by choosing a final subset of features used by the machine learning model to make classification decisions.\n",
    "\n",
    "To help us determine the impact of genetic algorithm in the feature selection, we combine this method with the decision tree technique applied to tabular data. \n",
    "\n",
    "The following subsections describe the decision tree model in general, the tabular data we chose, and the metrics we measure to evaluate the genetic algorithm in feature selection.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D0cXeDiuy2xx"
   },
   "source": [
    "\n",
    "## 3.1 - Decision Tree\n",
    "\n",
    "A decision tree is a flowchart-like structure in which each internal node represents a test on a feature. In the figure below, \"is the income over or below $30,000?\" is a test performed on the feature called \"Income\" for instance. Each branch represents the outcome of the test, and each leaf node represents a class label. A decision about the class predicted is taken after performing a series of test on features, until reaching a leaf node. The paths from root to leaf represent classification rules.\n",
    "\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/lucaslzl/ga_ia_p2/master/images%20for%20report/decision%20tree%20example.png'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jv8RVuMf2xFL"
   },
   "source": [
    "## 3.2 - Tabular Data\n",
    "\n",
    "We use the term \"tabular data\" to designate datasets composed of features that could take numerical values (for instance the income amount of a client with a checking account in a bank institution) or categorical values (\"Yes\" or \"No\" values describing if a client has a membership in a mileage program from an airline company for instance). \n",
    "\n",
    "We chose to use only tabular data for practical reasons. They are relatively easy to preprocess and clean before feeding them to a decision tree for model building. This step is termed as the \"training\" part. Also, it will make it easier for us to interpret the results of feature selection performed by the genetic algorithm.\n",
    "\n",
    "In order for our project to be as close as possible to reality, the majority of the datasets we use come from the Kaggle website. Kaggle is an organization where people compete in building the best machine learning model for real world problems. \n",
    "\n",
    "The number of features from each dataset varies from 10 to over 500. That will help us evaluate the computing resources needed by the genetic algorithm applied to the decision tree technique, and its practicality in applying it to real world. In particular, as we will see in more detail in the results section, as we increase the number of features collected in a given dataset, the number of observations needed, to build a relatively \"good\" machine learning model, increases at an exponential rate. This issue is termed as \"the curse of dimensionnality\". We mention here the importance in varying the number of features since it impacts tremendously the amount of resources needed by the genetic algorithm in terms of time and computing power.\n",
    "\n",
    "The description of the datasets we selected are provided as links to the sources where the datasets are encountered. The full list is provided in the last section of this report called \"Source\". In addition, the datasets can consulted in the following folder: https://github.com/lucaslzl/ga_ia_p2/tree/master/data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CMz2AAT93WtU"
   },
   "source": [
    "## 3.3 - Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wZ3sRxABqRwC"
   },
   "source": [
    "# 4 - Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z5OncdpEqRwC"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2eiLiZKJqRwD"
   },
   "source": [
    "# 5 - Conclusions\n",
    "\n",
    "Points to discuss:\n",
    "1. decision tree model is relatively easy to implement and can be a first choice for machine learning model building. If the results are limited, then more complex models can be explored using also genetic algorithm. \n",
    "2. decision tree can be used to filter the features for building more complex models and discarding features giving redundant information about the target. Thus helping saving time along the way of building heavy machine learning models.\n",
    "3. as we increase the number of features (talk about the \"curse of dimensionality\"), genetic algorithm requires increasing time. But it's a step that is executed only once.\n",
    "4. genetic algorithm was applied to classification problems, but can be applied to regression and to features not necessarily tabular. Other types of feaures could be images, texts, etc.\n",
    "5. comparison with Principa Component Analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jBA1c9DHqRwD"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CE35siLs3ICj"
   },
   "source": [
    "# Sources\n",
    "\n",
    "*Datasets used in the project:*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Vsyf19A0q2sQ"
   ],
   "name": "report.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
